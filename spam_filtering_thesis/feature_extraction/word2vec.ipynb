{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wx1kkTR8zWyk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698248751942,"user_tz":-180,"elapsed":3987,"user":{"displayName":"Basilis Sakellariou","userId":"01245548050423359889"}},"outputId":"9f7ac461-18c2-496e-aaa5-064619f9b866"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["# import libraries\n","\n","import gensim.downloader\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4ZtbWaf0irB"},"outputs":[],"source":["# load the data\n","\n","sms = pd.read_csv('/content/drive/MyDrive/spam_detection/sms_translate.csv') # load spam dataset\n","\n","enron = pd.read_csv('/content/drive/MyDrive/spam_detection/enron_full.csv')\n","\n","youtube = pd.read_csv('/content/drive/MyDrive/spam_detection/youtube_translate.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kit103-3zfJN"},"outputs":[],"source":["# Prepare the dataset with tokenizer that separates words by \" \". also seperates the punctuation from the words if we keep them\n","# Secondly do some cleaning\n","\n","en_stop_words = stopwords.words('english')\n","gr_stop_words = stopwords.words('greek')\n","\n","def cleaning_en(text):\n","\n","  text = text.lower()\n","  tokens = word_tokenize(text)\n","  # tokens = [token for token in tokens if token not in en_stop_words] # remove stop words\n","\n","  return tokens\n","\n","def cleaning_gr(text):\n","\n","  text = text.lower()\n","  tokens = word_tokenize(text)\n","  # tokens = [token for token in tokens if token not in gr_stop_words] # remove stop words\n","\n","  return tokens\n"]},{"cell_type":"code","source":["# # preprocessing\n","# sms['tokenize_en'] = sms.Message.apply(cleaning_en)\n","# sms['tokenize_gr'] = sms.gtrans_el.apply(cleaning_gr)\n","\n","enron['tokenize_en'] = enron.Message.apply(cleaning_en)\n","enron['tokenize_gr'] = enron.gtrans_el.apply(cleaning_gr)\n","\n","# twitter['tokenize_en'] = twitter.Message.apply(cleaning_en)\n","# twitter['tokenize_gr'] = twitter.gtrans_el.apply(cleaning_gr)\n","\n","# youtube['tokenize_en'] = youtube.Message.apply(cleaning_en)\n","# youtube['tokenize_gr'] = youtube.gtrans_el.apply(cleaning_gr)"],"metadata":{"id":"elfTSZMxMMdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flag = 'gr' # choose the language\n","data = enron # choose the dataset for training\n","name_data = 'enron'\n","\n","if flag == 'en':\n","\n","  # english data\n","  X = data.tokenize_en\n","  y = data.Category.values\n","\n","else:\n","\n","  # greek data\n","  X = data.tokenize_gr\n","  y = data.Category.values"],"metadata":{"id":"StZXqz9bcQ5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EVALUATION ON 20% OF DATASET\n","# validation on 20% of dataset to tune hyperparameters\n","# data split\n","# training data for Word2Vec must be list of lists\n","\n","Xtrain, Xtest,ytrain, ytest = train_test_split(X.tolist(), y, random_state=56, test_size=0.2, stratify = y)\n","x_train, x_valid,y_train, y_valid = train_test_split(Xtrain, ytrain, random_state=56, test_size=0.25, stratify = ytrain)"],"metadata":{"id":"La8qSgN0MKR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Word2vec implementation"],"metadata":{"id":"WpNq-WogEjfq"}},{"cell_type":"code","source":["# for tuning the hyperparameters\n","\n","# Xtrain = x_train\n","# Xtest = x_valid\n","# ytest = y_valid\n","# ytrain = y_train"],"metadata":{"id":"qA3-pQOfZ2mn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DI7yd-JoztCC"},"outputs":[],"source":["# train the Word2vec model, we use CBOW model sg = 0\n","vec_size = 200 # size of vector embeddings for each sentence\n","model = Word2Vec(sentences=Xtrain,epochs = 5, window = 10, min_count=5, vector_size=vec_size, sg=0)\n","\n","### text vectorization\n","\n","vocab = model.wv.index_to_key # vocabulary words\n","\n","# we will acquire the embedding with traversing through data (data: list of lists of words)\n","\n","def text_vectorization(data):\n","\n","    sentences_emb = np.zeros((len(data),vec_size)) # vector embeddings of given dataset\n","    for i in range(len(data)):\n","        sentence = data[i]\n","        w_embeddings = [] # here we store the embedding of each word in a sentence\n","        for words in sentence:\n","        # do this when we use word2vec\n","            if words in vocab:\n","                w_embeddings.append(model.wv[words])\n","            else:\n","               w_embeddings.append([0]*vec_size) # if word not in vocab vector has zeros, we can also skip the OOV words\n","        sentences_emb[i,:] = np.sum(w_embeddings,axis=0) # calculate the mean or sum to create a vector for each sentence\n","\n","    return sentences_emb\n","\n","# encoding as numpy array to use it in sklearn models\n","\n","Xtrain_vec = text_vectorization(Xtrain) # input: list of lists of word, Output: embeddings of sentences as numpy array\n","Xtest_vec = text_vectorization(Xtest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyXb6W3C0H7B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698251545021,"user_tz":-180,"elapsed":469583,"user":{"displayName":"Basilis Sakellariou","userId":"01245548050423359889"}},"outputId":"96b59a64-1816-4ed6-caff-ea88f9fb0f79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset used for training purpose is:  enron\n","Classifier:LogisticRegression(max_iter=10000, random_state=56, solver='sag') -  F1 Macro:0.9637\n","Classifier:DecisionTreeClassifier(random_state=56) -  F1 Macro:0.93\n","Classifier:SVC(random_state=56) -  F1 Macro:0.9671\n","Classifier:RandomForestClassifier(n_estimators=150, n_jobs=-1, random_state=56) -  F1 Macro:0.9729\n","Classifier:LogisticRegression(max_iter=10000, random_state=56, solver='sag') -  Accuracy:0.9639\n","Classifier:DecisionTreeClassifier(random_state=56) -  Accuracy:0.9303\n","Classifier:SVC(random_state=56) -  Accuracy:0.9671\n","Classifier:RandomForestClassifier(n_estimators=150, n_jobs=-1, random_state=56) -  Accuracy:0.973\n","Classifier:LogisticRegression(max_iter=10000, random_state=56, solver='sag') -  BalancedAccuracy:0.9637\n","Classifier:DecisionTreeClassifier(random_state=56) -  BalancedAccuracy:0.9298\n","Classifier:SVC(random_state=56) -  BalancedAccuracy:0.9677\n","Classifier:RandomForestClassifier(n_estimators=150, n_jobs=-1, random_state=56) -  BalancedAccuracy:0.973\n"]}],"source":["# after vectorization\n","# train and evaluate different machine learning algorithms\n","# evaluation metrics accuracy, f1 macro, balanced accuracy\n","# Logistic Regression, Decision tree, SVM, Random Forest\n","# the ensemble method Random Forest decrease the propability that Decision tree has to overfit in training data\n","\n","# solver = 'sag' for logistic regression on enron, due to large dataset also max_iter is set to 10000 because of converge problem of solver\n","models = [LogisticRegression(solver='sag',random_state=56,max_iter=10000), DecisionTreeClassifier(random_state=56), SVC(random_state=56),\n","          RandomForestClassifier(n_estimators=150,n_jobs=-1,random_state=56)]\n","\n","f_measures = {}\n","acc = {}\n","balanced_acc = {}\n","\n","for clf in models:\n","  clf.fit(Xtrain_vec,ytrain)\n","  pred = clf.predict(Xtest_vec)\n","  key = f'{clf}'\n","  f_measures[key]=f1_score(ytest, pred, average='macro')\n","  acc[key] = accuracy_score(ytest,pred)\n","  balanced_acc[key] = balanced_accuracy_score(ytest,pred)\n","\n","\n","print(\"Dataset used for training purpose is: \",name_data)\n","for name,score in f_measures.items():\n","    print(\"Classifier:{} -  F1 Macro:{}\".format(name,round(score,4)))\n","for name,score in acc.items():\n","    print(\"Classifier:{} -  Accuracy:{}\".format(name,round(score,4)))\n","for name,score in balanced_acc.items():\n","    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZOx5+TJj7UGgKuAOREZYR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}