{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3672,"status":"ok","timestamp":1698264979730,"user":{"displayName":"Vasilieios Sakellariou","userId":"02051358512126511037"},"user_tz":-180},"id":"54-28AanXDV5","outputId":"415db147-2f41-4922-a4a7-17edc7a192a4"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# import libraries\n","\n","import gensim.downloader\n","from gensim.models import FastText\n","import numpy as np\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAsyAeFUXPyf"},"outputs":[],"source":["# load the data\n","\n","sms = pd.read_csv('/content/drive/MyDrive/spam_detection/sms_translate.csv') # load spam dataset\n","\n","enron = pd.read_csv('/content/drive/MyDrive/spam_detection/enron_full.csv')\n","\n","youtube = pd.read_csv('/content/drive/MyDrive/spam_detection/youtube_translate.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRB_DrERXUNB"},"outputs":[],"source":["# Prepare the dataset with tokenizer that separates words by \" \". also seperates the punctuation from the words if we keep them\n","# Secondly do some cleaning\n","\n","en_stop_words = stopwords.words('english')\n","\n","gr_stop_words = stopwords.words('greek')\n","\n","def cleaning_en(text):\n","\n","  text = text.lower()\n","  tokens = word_tokenize(text)\n","  # tokens = [token for token in tokens if token not in en_stop_words] # remove stop words\n","\n","  return tokens\n","\n","def cleaning_gr(text):\n","\n","  text = text.lower()\n","  tokens = word_tokenize(text)\n","  # tokens = [token for token in tokens if token not in gr_stop_words] # remove stop words\n","\n","  return tokens\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znCAd8QPKkPh"},"outputs":[],"source":["# preprocessing\n","\n","# sms['tokenize_en'] = sms.Message.apply(cleaning_en)\n","# sms['tokenize_gr'] = sms.gtrans_el.apply(cleaning_gr)\n","\n","\n","enron['tokenize_en'] = enron.Message.apply(cleaning_en)\n","enron['tokenize_gr'] = enron.gtrans_el.apply(cleaning_gr)\n","\n","\n","# twitter['tokenize_en'] = twitter.Message.apply(cleaning_en)\n","# twitter['tokenize_gr'] = twitter.gtrans_el.apply(cleaning_gr)\n","\n","\n","# youtube['tokenize_en'] = youtube.Message.apply(cleaning_en)\n","# youtube['tokenize_gr'] = youtube.gtrans_el.apply(cleaning_gr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3aUlHxltDGM"},"outputs":[],"source":["flag = 'gr' # choose the language\n","data = enron # choose the dataset for training\n","name_data = 'enron'\n","\n","if flag == 'en':\n","\n","  # english data\n","  X = data.tokenize_en\n","  y = data.Category.values\n","\n","else:\n","\n","  # greek data\n","  X = data.tokenize_gr\n","  y = data.Category.values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvZ9rQl4K0Pd"},"outputs":[],"source":["# EVALUATION ON 20% OF DATASET\n","# validation on 20% of dataset to tune hyperparameters\n","# data split\n","Xtrain, Xtest,ytrain, ytest = train_test_split(X.tolist(), y, random_state=56, test_size=0.2, stratify = y)\n","x_train, x_valid,y_train, y_valid = train_test_split(Xtrain, ytrain, random_state=56, test_size=0.25, stratify = ytrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1pKIF_82B5U"},"outputs":[],"source":["# for tuning the hyperparameters\n","\n","# Xtrain = x_train\n","# Xtest = x_valid\n","# ytest = y_valid\n","# ytrain = y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpR45oSyXf44"},"outputs":[],"source":["# train the FastText model\n","# FastText takes into account of n-gramm of the word to produce the embedding of word\n","# FastText model take cares of out of vocabulary words\n","vec_size = 200\n","model = FastText(sentences=Xtrain,epochs=5,min_count=5,vector_size=vec_size,min_n=3,max_n=6,window=5)\n","\n","### text vectorization\n","def text_vectorization(data,vec_size):\n","\n","    sentences_emb = np.zeros((len(data),vec_size)) # vector embeddings of given dataset\n","    for i in range(len(data)):\n","        sentence = data[i]\n","        w_embeddings = [] # here we store the embedding of each word in a sentence\n","        for words in sentence:\n","           w_embeddings.append(model.wv[words])\n","        sentences_emb[i,:] = np.sum(w_embeddings,axis=0) # calculate the mean or sum to create a vector for each sentence\n","\n","    return sentences_emb\n","\n","Xtrain = text_vectorization(Xtrain,vec_size) # input list of lists of word and we take the sentence embeddings\n","Xtest = text_vectorization(Xtest,vec_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjKVMMOEXiLA","executionInfo":{"status":"ok","timestamp":1698268410202,"user_tz":-180,"elapsed":343985,"user":{"displayName":"Vasilieios Sakellariou","userId":"02051358512126511037"}},"outputId":"cda305c6-e812-4906-adbb-68c3281dcec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset used for training purpose is:  enron\n","Classifier:LogisticRegression(max_iter=5000, random_state=56, solver='sag') -  F1 Macro:0.953\n","Classifier:LogisticRegression(max_iter=5000, random_state=56, solver='sag') -  Accuracy:0.9531\n","Classifier:LogisticRegression(max_iter=5000, random_state=56, solver='sag') -  BalancedAccuracy:0.9533\n"]}],"source":["# after vectorization\n","# train and evaluate different machine learning algorithms\n","# evaluation metrics accuracy, f1 macro, balanced accuracy\n","# Logistic Regression, Decision tree, SVM, Random Forest\n","# the ensemble method Random Forest decrease the propability that Decision tree has to overfit in training data\n","\n","# use weight class when we have imbalanced data such as sms dataset\n","# models = [LogisticRegression(solver='liblinear',class_weight='balanced',random_state=56), DecisionTreeClassifier(class_weight='balanced',random_state=56), SVC(class_weight='balanced',random_state=56),\n","#           RandomForestClassifier(n_estimators=150,n_jobs=-1, class_weight='balanced',random_state=56)]\n","\n","# solver = 'sag' for logistic regression on enron, due to large dataset also max_iter is set to 5000 because of converge problem of solver\n","models = [LogisticRegression(solver='sag',random_state=56,max_iter=5000), DecisionTreeClassifier(random_state=56), SVC(random_state=56),\n","          RandomForestClassifier(n_estimators=150,n_jobs=-1,random_state=56)]\n","\n","f_measures = {}\n","acc = {}\n","balanced_acc = {}\n","\n","for clf in models:\n","  clf.fit(Xtrain,ytrain)\n","  pred = clf.predict(Xtest)\n","  key = f'{clf}'\n","  f_measures[key]=f1_score(ytest, pred, average='macro')\n","  acc[key] = accuracy_score(ytest,pred)\n","  balanced_acc[key] = balanced_accuracy_score(ytest,pred)\n","\n","\n","print(\"Dataset used for training purpose is: \",name_data)\n","for name,score in f_measures.items():\n","    print(\"Classifier:{} -  F1 Macro:{}\".format(name,round(score,4)))\n","for name,score in acc.items():\n","    print(\"Classifier:{} -  Accuracy:{}\".format(name,round(score,4)))\n","for name,score in balanced_acc.items():\n","    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}